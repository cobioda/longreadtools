# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/Isomatrix_tools.ipynb.

# %% auto 0
__all__ = ['isomatrix_to_anndata', 'download_test_data', 'simulate_isomatrix', 'simulate_and_save_isomatrices',
           'convert_and_save_file', 'multiple_isomatrix_conversion', 'load_and_set_var_names',
           'feature_set_standardization', 'concatenate_anndata']

# %% ../nbs/Isomatrix_tools.ipynb 3
import pandas as pd
import scanpy as sc
from scanpy import AnnData
from scipy.sparse import csr_matrix
import warnings

def isomatrix_to_anndata(file_path:str,  # The path to the isomatrix csv file to be read.
                        sparse:bool=True  # Flag to determine if the output should be a sparse matrix.
) -> AnnData: # The converted isomatrix as a scanpy compatible  anndata object
    """
    This function converts an isomatrix txt file (SiCeLoRe output) into an AnnData object compatible with scanpy

    """
    
    # Read in the data from the file
    df = pd.read_csv(file_path, sep='\t', index_col=0)
    # Filter out rows where the transcriptId is "undef"
    df = df.loc[df["transcriptId"] != "undef"]
    
    df = df.reset_index()
    df = df.transpose()
    
    # Extract the rows with 'gene_id', 'transcript_id', 'nb_exons' from the DataFrame
    additional_info_rows = df.loc[df.index.intersection(['geneId', 'transcriptId', 'nbExons'])]
    # Drop 'gene_id', 'transcript_id', 'nb_exons' rows from the DataFrame if they exist
    df = df.drop(['geneId', 'transcriptId', 'nbExons'], errors='ignore')

    # Convert the DataFrame to a sparse matrix if the sparse flag is True
    if sparse:
        matrix = csr_matrix(df.values.astype('float32'))
    else:
        try:
            matrix = df.values.astype('float32')
        except ValueError:
            print("Error: Non-numeric data present in the DataFrame. Cannot convert to float.")
            return None
    
    # Convert the matrix to an AnnData object
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        anndata = sc.AnnData(X=matrix, obs=pd.DataFrame(index=df.index), var=pd.DataFrame(index=df.columns))
    
    # Add additional information to the AnnData object vars
    for info in ['geneId', 'transcriptId', 'nbExons']:
        if info in additional_info_rows.index:
            anndata.var[info] = additional_info_rows.loc[info, :].values
            if info == 'nbExons':
                anndata.var[info] = anndata.var[info].astype('int32')
    
    return anndata

# %% ../nbs/Isomatrix_tools.ipynb 4
def download_test_data() -> str: #The absolute path of the extracted file 'sample_isomatrix.txt' if the download is successful.
    """
    This function downloads a test data file from a specified URL, saves it locally, and extracts it.
    """
    import urllib.request
    import gzip
    import shutil
    import os

    # URL of the file to be downloaded
    url = "https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748087/suppl/GSM3748087%5F190c.isoforms.matrix.txt.gz"

    # Download the file from `url` and save it locally under `file.txt.gz`:
    urllib.request.urlretrieve(url, 'file.txt.gz')

    # Check if the file is downloaded correctly
    if os.path.exists('file.txt.gz'):
        print("File downloaded successfully")
        # Now we need to extract the file
        with gzip.open('file.txt.gz', 'rb') as f_in:
            with open('sample_isomatrix.txt', 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        print("File extracted successfully")
        return os.path.abspath('sample_isomatrix.txt')
    else:
        print("Failed to download the file")
        return None


# %% ../nbs/Isomatrix_tools.ipynb 13
import numpy as np 
from pandas import DataFrame

def simulate_isomatrix(num_genes: int, # number of genes (groups of rows)
                       num_transcripts_per_gene: int, # number of transcripts per gene
                       num_samples: int, # number of samples (columns)
                       sparsity: float = 0.95, # fraction of zeros in the data (default 0.95)
                       max_expression: int = 100, # maximum expression level for any transcript in any sample
                       seed: int = 0 # random seed for reproducibility
                      ) -> DataFrame : # DataFrame with simulated transcript expression data for demonstration purposes.
    """
    Simulate transcript expression data to match the structure of the first image provided by the user.
    Allows specifying the number of genes, transcripts per gene, and samples.
    """
    # Set random seed for reproducibility
    np.random.seed(seed)
    
    # Calculate total number of transcripts
    total_transcripts = num_genes * num_transcripts_per_gene
    
    # Generate random data
    data = np.random.rand(total_transcripts, num_samples)
    
    # Apply sparsity
    zero_mask = np.random.rand(total_transcripts, num_samples) > sparsity
    data[~zero_mask] = 0  # Set a fraction of data to 0 based on sparsity
    
    # Scale data to have values up to max_expression
    data = np.ceil(data * max_expression).astype(int)
    
    # Generate transcript and sample labels
    transcript_ids = [f"ENSMUST00000{str(i).zfill(6)}_{str(j).zfill(6)}_{str(seed).zfill(6)}.1" for j in range(num_genes) for i in range(1, num_transcripts_per_gene + 1)]
    gene_ids = [f"Gene_{(i // num_transcripts_per_gene) + 1}" for i in range(total_transcripts)]
    nb_exons = np.random.randint(1, 21, total_transcripts)  # Assuming 1-20 exons based on typical gene structures
    sample_ids = [f"CACCTACACGTCAAC{str(i).zfill(2)}" for i in range(1, num_samples + 1)]
    
    # Create DataFrame
    df = pd.DataFrame(data, index=gene_ids, columns=sample_ids)
    df.index.name = 'geneId'  # Add index name
    df.insert(0, 'transcriptId', transcript_ids)
    df.insert(1, 'nbExons', nb_exons)
    
    return df


# %% ../nbs/Isomatrix_tools.ipynb 14
import os
def simulate_and_save_isomatrices(num_isomatrix: int, #number of isomatrix to generate
                                num_genes: int, # number of genes (groups of rows)
                                num_transcripts_per_gene: int, # number of transcripts per gene
                                num_samples: int, # number of samples (columns)
                                sparsity: float = 0.95, # fraction of zeros in the data (default 0.95)
                                max_expression: int = 100, # maximum expression level for any transcript in any sample
                                seed: int = 0, # random seed for reproducibility
                                output_dir: str = './', # directory to save the generated isomatrix txt files
                                return_paths: bool = False, # return paths to the isomatrixs as a list of strings if True
                                verbose: bool = False # print progress messages if True
                               ) -> list: # list of paths for the simulated matrices (if return is set True)
    
    """
    Simulate multiple isomatrix and save them as txt files in the specified directory.
    If return_paths is True, return a list of paths to the saved isomatrix files.
    """
    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)
    
    output_files = []
    for i in range(num_isomatrix):
        # Generate isomatrix
        df = simulate_isomatrix(num_genes, num_transcripts_per_gene, num_samples, sparsity, max_expression, seed+i)
        
        # Save to txt file
        output_file = os.path.join(output_dir, f'isomatrix_{i+1}.txt')
        df.to_csv(output_file, sep='\t')
        
        if verbose:
            print(f'Isomatrix {i+1} saved to {output_file}')
        output_files.append(output_file)
    
    if return_paths:
        return output_files

# %% ../nbs/Isomatrix_tools.ipynb 15
import os
import time

def convert_and_save_file(sample, verbose, sparse=False):
    anndata = isomatrix_to_anndata(sample, sparse=sparse)
    h5ad_file = sample.replace('.txt', '.h5ad')
    
    # Check if the file already exists and delete it if it does
    if os.path.exists(h5ad_file):
        os.remove(h5ad_file)
    
    # Add a delay and retry mechanism to handle file locking issues
    for attempt in range(10):
        try:
            anndata.write_h5ad(h5ad_file)
            break
        except BlockingIOError:
            if attempt < 9:  # i.e. if this is not the last attempt
                time.sleep(1)  # wait for 1 second before the next attempt
            else:
                raise  # re-raise the last exception if all attempts fail
    
    if verbose:
        print(f"File {h5ad_file} was successfully written to disk.")
    
    return h5ad_file




# %% ../nbs/Isomatrix_tools.ipynb 16
from multiprocessing import Pool
import os
from functools import partial


def multiple_isomatrix_conversion(file_paths: list, # A list of file paths to be converted.
                                  verbose: bool = False, # If True, print progress messages.
                                  return_paths: bool = False, # If True, return a list of paths to the converted files.
                                  sparse: bool = False # If True, the anndata object will be stored in sparse format.
                                  ) -> list: # A list of paths to the converted files.
    """
    This function takes a list of file paths, converts each file from isomatrix to anndata format, 
    and saves the converted file in the same location with the same name but with a .h5ad extension.
    If return_paths is True, it returns a list of paths to the converted files.
    If sparse is True, the anndata object will be stored in sparse format.
    """
    with Pool() as p:
        converted_files = p.map(partial(convert_and_save_file, verbose=verbose, sparse=sparse), file_paths)
    
    if return_paths:
        return converted_files

# %% ../nbs/Isomatrix_tools.ipynb 19
from joblib import Parallel, delayed
from tqdm import tqdm

def load_and_set_var_names(path):
    dataset = sc.read_h5ad(path, backed='r')  # Read the file in 'backed' mode to avoid loading the whole data into memory
    dataset.var_names = dataset.var['transcriptId']
    return dataset

# %% ../nbs/Isomatrix_tools.ipynb 23
def feature_set_standardization(adatas:list, # list of AnnData objects or paths to AnnData files
                                standardization_method:str = 'union' # str specifiying method to use union or intersection
                                ) -> list: # list of anndata objects with the features standardised 
    """
    Standardize the feature sets of multiple AnnData objects.
    
    This function takes a list of AnnData objects or paths to AnnData files and a standardization method as input.
    The standardization method can be either 'union' or 'intersection'.
    If 'union' is chosen, the function will create a union of all features across all AnnData objects.
    If 'intersection' is chosen, the function will create an intersection of all features across all AnnData objects.
    The function returns a list of standardized AnnData objects.
    """
    # Check if the first element in adatas is a string
    if isinstance(adatas[0], str):
        # If it is, load anndata objects from paths
        adatas = [load_and_set_var_names(path) for path in adatas]

    all_features = set()
    common_features = set()
    
    # Get union or intersection of all feature sets across all AnnData objects
    if standardization_method == 'union':
        all_features = set().union(*[set(adata.var.itertuples(index=False, name=None)) for adata in adatas])
    elif standardization_method == 'intersection':
        common_features = set.intersection(*[set(adata.var['transcriptId']) for adata in adatas])
    else:
        raise ValueError("standardization_method should be 'union' or 'intersection'")
    

    standardized_adatas = []
    # Iterate over each AnnData object
    for dataset in tqdm(adatas, desc= f"Standardizing anndata features via {standardization_method}"):
        dataset.var_names = dataset.var['transcriptId']
        existing_var = dataset.var
        # Identify features that are in the union/intersection but not in the current AnnData object
        missing_features = all_features - set(dataset.var.itertuples(index=False, name=None))
        if standardization_method == 'union':
            if missing_features:
                # Create a DataFrame of zeros with rows equal to the number of observations in the current AnnData object
                # and columns equal to the number of missing features
                zero_data = np.zeros((dataset.n_obs, len(missing_features)), dtype=object)
                zero_df = pd.DataFrame(zero_data, index=dataset.obs_names, columns=pd.Index([t[1] for t in missing_features], name='transcriptId'))

                # Merge the zero_df with the dataset's .to_df() DataFrame along the columns
                combined_df = pd.concat([dataset.to_df(), zero_df], axis=1)

                # Convert the combined DataFrame back to an AnnData object
                combined_data = sc.AnnData(combined_df, obs=dataset.obs, var=pd.DataFrame(index=combined_df.columns))

                missing_df = pd.DataFrame(list(missing_features), columns=['geneId', 'transcriptId', 'nbExons'])
                missing_df.set_index('transcriptId', inplace=True)

                combined_data.var = pd.concat([existing_var, missing_df], axis=0)
                combined_data.var['transcriptId'] = combined_data.var_names
                standardized_adatas.append(combined_data)
            else:
                # If no features are missing, append the dataset as is
                standardized_adatas.append(dataset)
        elif standardization_method == 'intersection':
            # Subset the dataset to only include transcriptIds that are in the intersection
            dataset = dataset[:, dataset.var_names.isin(common_features)]
            standardized_adatas.append(dataset)
    return standardized_adatas


# %% ../nbs/Isomatrix_tools.ipynb 26
import anndata as ad

def concatenate_anndata(h5ad_inputs: list, # A list of AnnData objects or paths to .h5ad files.
                         standardization_method='union' # The method to standardize the feature sets across all AnnData objects. It can be either 'union' or 'intersection'. Default is 'union'.
                         ) -> AnnData: # The concatenated AnnData object.
    """
    This function concatenates multiple AnnData objects into a single AnnData object.
    """
    
    # Check if inputs are paths or actual anndata objects
    if isinstance(h5ad_inputs[0], str):
        # If inputs are paths, read the .h5ad files and generate batch keys based on the directory names
        to_concat = [sc.read_h5ad(input, backed='r') for input in h5ad_inputs]
        batch_keys = [os.path.basename(os.path.dirname(input)) for input in h5ad_inputs]
    else:
        # If inputs are AnnData objects, use them directly and generate unique batch keys for each dataset
        to_concat = h5ad_inputs
        batch_keys = [f"batch_{i}" for i, adata in enumerate(h5ad_inputs)]

    # Apply feature set standardization
    to_concat = feature_set_standardization(to_concat, standardization_method)

    # Ensure unique keys for concatenation
    if len(batch_keys) != len(set(batch_keys)):
        # If batch keys are not unique, create new unique batch keys
        batch_keys = [f"batch_{i}" for i in range(len(to_concat))]

    # Concatenate anndata objects
    concat_anndata = ad.concat(
        to_concat,
        join="outer",
        label='batch',
        keys=batch_keys,
    )

    # Set the .var attribute of the concatenated AnnData object to be the same as the first input AnnData object
    concat_anndata.var = to_concat[0].var

    return concat_anndata

