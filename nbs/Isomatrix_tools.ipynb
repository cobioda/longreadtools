{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isomatrix Tools\n",
    "\n",
    "> Tools for converting isomatrix files into anndata objects for integration with the Scanpy ecosystem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp isomatrix_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from scanpy import AnnData\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "\n",
    "def isomatrix_to_anndata(file_path:str,  # The path to the isomatrix csv file to be read.\n",
    "                        sparse:bool=True  # Flag to determine if the output should be a sparse matrix.\n",
    ") -> AnnData: # The converted isomatrix as a scanpy compatible  anndata object\n",
    "    \"\"\"\n",
    "    This function converts an isomatrix txt file (SiCeLoRe output) into an AnnData object compatible with scanpy\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read in the data from the file\n",
    "    df = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "    # Filter out rows where the transcriptId is \"undef\"\n",
    "    df = df.loc[df[\"transcriptId\"] != \"undef\"]\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    df = df.transpose()\n",
    "    \n",
    "    # Extract the rows with 'gene_id', 'transcript_id', 'nb_exons' from the DataFrame\n",
    "    additional_info_rows = df.loc[df.index.intersection(['geneId', 'transcriptId', 'nbExons'])]\n",
    "    # Drop 'gene_id', 'transcript_id', 'nb_exons' rows from the DataFrame if they exist\n",
    "    df = df.drop(['geneId', 'transcriptId', 'nbExons'], errors='ignore')\n",
    "\n",
    "    # Convert the DataFrame to a sparse matrix if the sparse flag is True\n",
    "    if sparse:\n",
    "        matrix = csr_matrix(df.values.astype('float32'))\n",
    "    else:\n",
    "        try:\n",
    "            matrix = df.values.astype('float32')\n",
    "        except ValueError:\n",
    "            print(\"Error: Non-numeric data present in the DataFrame. Cannot convert to float.\")\n",
    "            return None\n",
    "    \n",
    "    # Convert the matrix to an AnnData object\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        anndata = sc.AnnData(X=matrix, obs=pd.DataFrame(index=df.index), var=pd.DataFrame(index=df.columns))\n",
    "    \n",
    "    # Add additional information to the AnnData object vars\n",
    "    for info in ['geneId', 'transcriptId', 'nbExons']:\n",
    "        if info in additional_info_rows.index:\n",
    "            anndata.var[info] = additional_info_rows.loc[info, :].values\n",
    "            if info == 'nbExons':\n",
    "                anndata.var[info] = anndata.var[info].astype('int32')\n",
    "    \n",
    "    return anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_test_data() -> str: #The absolute path of the extracted file 'sample_isomatrix.txt' if the download is successful.\n",
    "    \"\"\"\n",
    "    This function downloads a test data file from a specified URL, saves it locally, and extracts it.\n",
    "    \"\"\"\n",
    "    import urllib.request\n",
    "    import gzip\n",
    "    import shutil\n",
    "    import os\n",
    "\n",
    "    # URL of the file to be downloaded\n",
    "    url = \"https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748087/suppl/GSM3748087%5F190c.isoforms.matrix.txt.gz\"\n",
    "\n",
    "    # Download the file from `url` and save it locally under `file.txt.gz`:\n",
    "    urllib.request.urlretrieve(url, 'file.txt.gz')\n",
    "\n",
    "    # Check if the file is downloaded correctly\n",
    "    if os.path.exists('file.txt.gz'):\n",
    "        print(\"File downloaded successfully\")\n",
    "        # Now we need to extract the file\n",
    "        with gzip.open('file.txt.gz', 'rb') as f_in:\n",
    "            with open('sample_isomatrix.txt', 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(\"File extracted successfully\")\n",
    "        return os.path.abspath('sample_isomatrix.txt')\n",
    "    else:\n",
    "        print(\"Failed to download the file\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of `isomatrix_to_anndata`: We can use the `download_test_data` function to download a small isoform matrix dataset for demonstrating the functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully\n",
      "File extracted successfully\n"
     ]
    }
   ],
   "source": [
    "from longreadtools.isomatrix_tools import * \n",
    "test_file = download_test_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "anndata = isomatrix_to_anndata(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the anndata object generated from the isomatrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneId</th>\n",
       "      <th>transcriptId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Klc2</td>\n",
       "      <td>ENSMUST00000156717.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Capn15</td>\n",
       "      <td>ENSMUST00000212520.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Klc2</td>\n",
       "      <td>ENSMUST00000025798.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eva1c</td>\n",
       "      <td>ENSMUST00000231280.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Atg5</td>\n",
       "      <td>ENSMUST00000039286.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20829</th>\n",
       "      <td>Kcnj9</td>\n",
       "      <td>ENSMUST00000062387.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20830</th>\n",
       "      <td>Iqcg</td>\n",
       "      <td>ENSMUST00000115100.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20831</th>\n",
       "      <td>Nt5dc2</td>\n",
       "      <td>ENSMUST00000227096.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20832</th>\n",
       "      <td>Emg1</td>\n",
       "      <td>ENSMUST00000004379.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20833</th>\n",
       "      <td>Kat6a</td>\n",
       "      <td>ENSMUST00000110696.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20834 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       geneId           transcriptId\n",
       "0        Klc2   ENSMUST00000156717.1\n",
       "1      Capn15   ENSMUST00000212520.1\n",
       "2        Klc2  ENSMUST00000025798.12\n",
       "3       Eva1c   ENSMUST00000231280.1\n",
       "4        Atg5   ENSMUST00000039286.4\n",
       "...       ...                    ...\n",
       "20829   Kcnj9   ENSMUST00000062387.7\n",
       "20830    Iqcg   ENSMUST00000115100.8\n",
       "20831  Nt5dc2   ENSMUST00000227096.1\n",
       "20832    Emg1   ENSMUST00000004379.7\n",
       "20833   Kat6a   ENSMUST00000110696.7\n",
       "\n",
       "[20834 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anndata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_isomatrix_to_anndata():\n",
    "    # Test with a known file\n",
    "    test_file = download_test_data()\n",
    "    anndata = isomatrix_to_anndata(test_file)\n",
    "\n",
    "    # Check the type of the returned object\n",
    "    assert isinstance(anndata, sc.AnnData), \"The returned object is not an AnnData object.\"\n",
    "\n",
    "    # Check the dimensions of the AnnData object\n",
    "    assert anndata.shape == (190, 20834), \"The dimensions of the AnnData object are not as expected.\"\n",
    "\n",
    "    # Check the var names of the AnnData object\n",
    "    assert 'geneId' in anndata.var, \"The 'geneId' is not in the var of the AnnData object.\"\n",
    "    assert 'transcriptId' in anndata.var, \"The 'transcriptId' is not in the var of the AnnData object.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully\n",
      "File extracted successfully\n"
     ]
    }
   ],
   "source": [
    "test_file = download_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, it may be necessary to convert more than one isomatrix in bulk. The function `multiple_isomatrix_conversion` has been designed for this purpose. It leverages Python's multiprocessing capabilities to perform this task in a fast and efficient manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np \n",
    "from pandas import DataFrame\n",
    "\n",
    "def simulate_isomatrix(num_genes: int, # number of genes (groups of rows)\n",
    "                       num_transcripts_per_gene: int, # number of transcripts per gene\n",
    "                       num_samples: int, # number of samples (columns)\n",
    "                       sparsity: float = 0.95, # fraction of zeros in the data (default 0.95)\n",
    "                       max_expression: int = 100, # maximum expression level for any transcript in any sample\n",
    "                       seed: int = 0 # random seed for reproducibility\n",
    "                      ) -> DataFrame : # DataFrame with simulated transcript expression data for demonstration purposes.\n",
    "    \"\"\"\n",
    "    Simulate transcript expression data to match the structure of the first image provided by the user.\n",
    "    Allows specifying the number of genes, transcripts per gene, and samples.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Calculate total number of transcripts\n",
    "    total_transcripts = num_genes * num_transcripts_per_gene\n",
    "    \n",
    "    # Generate random data\n",
    "    data = np.random.rand(total_transcripts, num_samples)\n",
    "    \n",
    "    # Apply sparsity\n",
    "    zero_mask = np.random.rand(total_transcripts, num_samples) > sparsity\n",
    "    data[~zero_mask] = 0  # Set a fraction of data to 0 based on sparsity\n",
    "    \n",
    "    # Scale data to have values up to max_expression\n",
    "    data = np.ceil(data * max_expression).astype(int)\n",
    "    \n",
    "    # Generate transcript and sample labels\n",
    "    transcript_ids = [f\"ENSMUST00000{str(i).zfill(6)}_{str(j).zfill(6)}_{str(seed).zfill(6)}.1\" for j in range(num_genes) for i in range(1, num_transcripts_per_gene + 1)]\n",
    "    gene_ids = [f\"Gene_{(i // num_transcripts_per_gene) + 1}\" for i in range(total_transcripts)]\n",
    "    nb_exons = np.random.randint(1, 21, total_transcripts)  # Assuming 1-20 exons based on typical gene structures\n",
    "    sample_ids = [f\"CACCTACACGTCAAC{str(i).zfill(2)}\" for i in range(1, num_samples + 1)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, index=gene_ids, columns=sample_ids)\n",
    "    df.index.name = 'geneId'  # Add index name\n",
    "    df.insert(0, 'transcriptId', transcript_ids)\n",
    "    df.insert(1, 'nbExons', nb_exons)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "def simulate_and_save_isomatrices(num_isomatrix: int, #number of isomatrix to generate\n",
    "                                num_genes: int, # number of genes (groups of rows)\n",
    "                                num_transcripts_per_gene: int, # number of transcripts per gene\n",
    "                                num_samples: int, # number of samples (columns)\n",
    "                                sparsity: float = 0.95, # fraction of zeros in the data (default 0.95)\n",
    "                                max_expression: int = 100, # maximum expression level for any transcript in any sample\n",
    "                                seed: int = 0, # random seed for reproducibility\n",
    "                                output_dir: str = './', # directory to save the generated isomatrix txt files\n",
    "                                return_paths: bool = False, # return paths to the isomatrixs as a list of strings if True\n",
    "                                verbose: bool = False # print progress messages if True\n",
    "                               ) -> list: # list of paths for the simulated matrices (if return is set True)\n",
    "    \n",
    "    \"\"\"\n",
    "    Simulate multiple isomatrix and save them as txt files in the specified directory.\n",
    "    If return_paths is True, return a list of paths to the saved isomatrix files.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_files = []\n",
    "    for i in range(num_isomatrix):\n",
    "        # Generate isomatrix\n",
    "        df = simulate_isomatrix(num_genes, num_transcripts_per_gene, num_samples, sparsity, max_expression, seed+i)\n",
    "        \n",
    "        # Save to txt file\n",
    "        output_file = os.path.join(output_dir, f'isomatrix_{i+1}.txt')\n",
    "        df.to_csv(output_file, sep='\\t')\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Isomatrix {i+1} saved to {output_file}')\n",
    "        output_files.append(output_file)\n",
    "    \n",
    "    if return_paths:\n",
    "        return output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "import os\n",
    "import time\n",
    "\n",
    "def convert_and_save_file(sample, verbose, sparse=False):\n",
    "    anndata = isomatrix_to_anndata(sample, sparse=sparse)\n",
    "    h5ad_file = sample.replace('.txt', '.h5ad')\n",
    "    \n",
    "    # Check if the file already exists and delete it if it does\n",
    "    if os.path.exists(h5ad_file):\n",
    "        os.remove(h5ad_file)\n",
    "    \n",
    "    # Add a delay and retry mechanism to handle file locking issues\n",
    "    for attempt in range(10):\n",
    "        try:\n",
    "            anndata.write_h5ad(h5ad_file)\n",
    "            break\n",
    "        except BlockingIOError:\n",
    "            if attempt < 9:  # i.e. if this is not the last attempt\n",
    "                time.sleep(1)  # wait for 1 second before the next attempt\n",
    "            else:\n",
    "                raise  # re-raise the last exception if all attempts fail\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"File {h5ad_file} was successfully written to disk.\")\n",
    "    \n",
    "    return h5ad_file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def multiple_isomatrix_conversion(file_paths: list, # A list of file paths to be converted.\n",
    "                                  verbose: bool = False, # If True, print progress messages.\n",
    "                                  return_paths: bool = False, # If True, return a list of paths to the converted files.\n",
    "                                  sparse: bool = False # If True, the anndata object will be stored in sparse format.\n",
    "                                  ) -> list: # A list of paths to the converted files.\n",
    "    \"\"\"\n",
    "    This function takes a list of file paths, converts each file from isomatrix to anndata format, \n",
    "    and saves the converted file in the same location with the same name but with a .h5ad extension.\n",
    "    If return_paths is True, it returns a list of paths to the converted files.\n",
    "    If sparse is True, the anndata object will be stored in sparse format.\n",
    "    \"\"\"\n",
    "    with Pool() as p:\n",
    "        converted_files = p.map(partial(convert_and_save_file, verbose=verbose, sparse=sparse), file_paths)\n",
    "    \n",
    "    if return_paths:\n",
    "        return converted_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to use the function to convert multiple Isomatrix objects simultaneously. The function requires a list of paths to the Isomatrix text files as input. To demonstrate this functionality and to aid further development by other contributors, the ability to simulate the Isomatrix data has been provided.\n",
    " \n",
    "In this section, we will be making use of the `simulate_and_save_isomatrices` function to generate and store 10 isomatrices. Following this, the `multiple_isomatrix_conversion` function will be employed to convert these isomatrices into anndata objects and subsequently save them to the disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# import os\n",
    "# import re\n",
    "\n",
    "# directory = '/data/analysis/data_mcandrew/000-sclr-discovair/'\n",
    "\n",
    "# # Define the regular expression pattern\n",
    "# pattern = re.compile('.*(_BIOP_INT|BIOP_NAS)$')\n",
    "\n",
    "# # Get a list of all files in the directory\n",
    "# all_files = os.listdir(directory)\n",
    "\n",
    "# # Filter the list to include only files that match the pattern\n",
    "# matching_files = [os.path.join(directory, f) for f in all_files if pattern.match(f)]\n",
    "\n",
    "# # Print the list of matching files\n",
    "# print(matching_files)\n",
    "\n",
    "# individual_runs = matching_files\n",
    "# individual_runs = [f'{run}_isomatrix.txt' for run in individual_runs]\n",
    "# individual_runs = [os.path.join(run, f'{os.path.basename(run)}_isomatrix.txt') for run in matching_files]\n",
    "\n",
    "# isomatrice_paths = individual_runs\n",
    "# anndata_paths = multiple_isomatrix_conversion(isomatrice_paths, verbose=True, return_paths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_set_var_names(path):\n",
    "    dataset = sc.read_h5ad(path, backed='r')  # Read the file in 'backed' mode to avoid loading the whole data into memory\n",
    "    dataset.var_names = dataset.var['transcriptId']\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's simulate and convert multiple isomatrices to showcase the functionality of our concatenation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isomatrix 1 saved to ./isomatrix_1.txt\n",
      "Isomatrix 2 saved to ./isomatrix_2.txt\n",
      "Isomatrix 3 saved to ./isomatrix_3.txt\n",
      "Isomatrix 4 saved to ./isomatrix_4.txt\n",
      "Isomatrix 5 saved to ./isomatrix_5.txt\n",
      "Isomatrix 6 saved to ./isomatrix_6.txt\n",
      "Isomatrix 7 saved to ./isomatrix_7.txt\n",
      "Isomatrix 8 saved to ./isomatrix_8.txt\n",
      "Isomatrix 9 saved to ./isomatrix_9.txt\n",
      "Isomatrix 10 saved to ./isomatrix_10.txt\n",
      "File ./isomatrix_5.h5ad was successfully written to disk.File ./isomatrix_2.h5ad was successfully written to disk.\n",
      "\n",
      "File ./isomatrix_3.h5ad was successfully written to disk.\n",
      "File ./isomatrix_7.h5ad was successfully written to disk.\n",
      "File ./isomatrix_8.h5ad was successfully written to disk.File ./isomatrix_10.h5ad was successfully written to disk.\n",
      "\n",
      "File ./isomatrix_6.h5ad was successfully written to disk.\n",
      "File ./isomatrix_1.h5ad was successfully written to disk.\n",
      "File ./isomatrix_9.h5ad was successfully written to disk.\n",
      "File ./isomatrix_4.h5ad was successfully written to disk.\n"
     ]
    }
   ],
   "source": [
    "simulation = simulate_and_save_isomatrices(num_isomatrix=10, num_genes=1000, num_transcripts_per_gene=5, num_samples=100, sparsity=0.95, max_expression=100, seed=0, output_dir='./', return_paths=True, verbose=True)\n",
    "anndata_paths = multiple_isomatrix_conversion(simulation, verbose=True, return_paths=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two other useful tools for working with isomatrix anndata. The first, `feature_set_standardization`, can take a list of isomatrix anndata objects and standardize the features. For instance, if we run the function with `standardization_method` set to 'union', it will return a list of anndata objects with the .var attribute being the union of all geneId, transcriptId, and nbExons. Any values missing in the original datasets will be set to zero. Alternatively, if we run it with `standardization_method` set to 'intersection', only the intersecting features will be retained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def feature_set_standardization(adatas:list, # list of AnnData objects or paths to AnnData files\n",
    "                                standardization_method:str = 'union' # str specifiying method to use union or intersection\n",
    "                                ) -> list: # list of anndata objects with the features standardised \n",
    "    \"\"\"\n",
    "    Standardize the feature sets of multiple AnnData objects.\n",
    "    \n",
    "    This function takes a list of AnnData objects or paths to AnnData files and a standardization method as input.\n",
    "    The standardization method can be either 'union' or 'intersection'.\n",
    "    If 'union' is chosen, the function will create a union of all features across all AnnData objects.\n",
    "    If 'intersection' is chosen, the function will create an intersection of all features across all AnnData objects.\n",
    "    The function returns a list of standardized AnnData objects.\n",
    "    \"\"\"\n",
    "    # Check if the first element in adatas is a string\n",
    "    if isinstance(adatas[0], str):\n",
    "        # If it is, load anndata objects from paths\n",
    "        adatas = [load_and_set_var_names(path) for path in adatas]\n",
    "\n",
    "    all_features = set()\n",
    "    common_features = set()\n",
    "    \n",
    "    # Get union or intersection of all feature sets across all AnnData objects\n",
    "    if standardization_method == 'union':\n",
    "        all_features = set().union(*[set(adata.var.itertuples(index=False, name=None)) for adata in adatas])\n",
    "    elif standardization_method == 'intersection':\n",
    "        common_features = set.intersection(*[set(adata.var['transcriptId']) for adata in adatas])\n",
    "    else:\n",
    "        raise ValueError(\"standardization_method should be 'union' or 'intersection'\")\n",
    "    \n",
    "\n",
    "    standardized_adatas = []\n",
    "    # Iterate over each AnnData object\n",
    "    for dataset in tqdm(adatas, desc= f\"Standardizing anndata features via {standardization_method}\"):\n",
    "        dataset.var_names = dataset.var['transcriptId']\n",
    "        existing_var = dataset.var\n",
    "        # Identify features that are in the union/intersection but not in the current AnnData object\n",
    "        missing_features = all_features - set(dataset.var.itertuples(index=False, name=None))\n",
    "        if standardization_method == 'union':\n",
    "            if missing_features:\n",
    "                # Create a DataFrame of zeros with rows equal to the number of observations in the current AnnData object\n",
    "                # and columns equal to the number of missing features\n",
    "                zero_data = np.zeros((dataset.n_obs, len(missing_features)), dtype=object)\n",
    "                zero_df = pd.DataFrame(zero_data, index=dataset.obs_names, columns=pd.Index([t[1] for t in missing_features], name='transcriptId'))\n",
    "\n",
    "                # Merge the zero_df with the dataset's .to_df() DataFrame along the columns\n",
    "                combined_df = pd.concat([dataset.to_df(), zero_df], axis=1)\n",
    "\n",
    "                # Convert the combined DataFrame back to an AnnData object\n",
    "                combined_data = sc.AnnData(combined_df, obs=dataset.obs, var=pd.DataFrame(index=combined_df.columns))\n",
    "\n",
    "                missing_df = pd.DataFrame(list(missing_features), columns=['geneId', 'transcriptId', 'nbExons'])\n",
    "                missing_df.set_index('transcriptId', inplace=True)\n",
    "\n",
    "                combined_data.var = pd.concat([existing_var, missing_df], axis=0)\n",
    "                combined_data.var['transcriptId'] = combined_data.var_names\n",
    "                standardized_adatas.append(combined_data)\n",
    "            else:\n",
    "                # If no features are missing, append the dataset as is\n",
    "                standardized_adatas.append(dataset)\n",
    "        elif standardization_method == 'intersection':\n",
    "            # Subset the dataset to only include transcriptIds that are in the intersection\n",
    "            dataset = dataset[:, dataset.var_names.isin(common_features)]\n",
    "            standardized_adatas.append(dataset)\n",
    "    return standardized_adatas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the function on are simulated data and see what the result looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standardizing anndata features via union:   0%|          | 0/10 [00:00<?, ?it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  10%|█         | 1/10 [00:00<00:05,  1.74it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  20%|██        | 2/10 [00:01<00:04,  1.76it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  30%|███       | 3/10 [00:01<00:03,  1.89it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  40%|████      | 4/10 [00:02<00:03,  1.93it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  50%|█████     | 5/10 [00:02<00:02,  1.95it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  60%|██████    | 6/10 [00:03<00:02,  1.99it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  70%|███████   | 7/10 [00:03<00:01,  2.02it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  80%|████████  | 8/10 [00:04<00:01,  1.98it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  90%|█████████ | 9/10 [00:04<00:00,  2.00it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union: 100%|██████████| 10/10 [00:05<00:00,  1.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AnnData object with n_obs × n_vars = 100 × 50000\n",
       "     var: 'geneId', 'transcriptId', 'nbExons',\n",
       " AnnData object with n_obs × n_vars = 100 × 50000\n",
       "     var: 'geneId', 'transcriptId', 'nbExons',\n",
       " AnnData object with n_obs × n_vars = 100 × 50000\n",
       "     var: 'geneId', 'transcriptId', 'nbExons',\n",
       " AnnData object with n_obs × n_vars = 100 × 50000\n",
       "     var: 'geneId', 'transcriptId', 'nbExons',\n",
       " AnnData object with n_obs × n_vars = 100 × 50000\n",
       "     var: 'geneId', 'transcriptId', 'nbExons',\n",
       " AnnData object with n_obs × n_vars = 100 × 50000\n",
       "     var: 'geneId', 'transcriptId', 'nbExons',\n",
       " AnnData object with n_obs × n_vars = 100 × 50000\n",
       "     var: 'geneId', 'transcriptId', 'nbExons',\n",
       " AnnData object with n_obs × n_vars = 100 × 50000\n",
       "     var: 'geneId', 'transcriptId', 'nbExons',\n",
       " AnnData object with n_obs × n_vars = 100 × 50000\n",
       "     var: 'geneId', 'transcriptId', 'nbExons',\n",
       " AnnData object with n_obs × n_vars = 100 × 50000\n",
       "     var: 'geneId', 'transcriptId', 'nbExons']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set_standardization(anndata_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import anndata as ad\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def concatenate_anndata(h5ad_inputs: list, # A list of AnnData objects or paths to .h5ad files.\n",
    "                         standardization_method='union', # The method to standardize the feature sets across all AnnData objects. It can be either 'union' or 'intersection'. Default is 'union'.\n",
    "                         sparse=False # Optional flag to convert the final matrix to sparse. Default is False.\n",
    "                         ) -> AnnData: # The concatenated AnnData object.\n",
    "    \"\"\"\n",
    "    This function concatenates multiple AnnData objects into a single AnnData object.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if inputs are paths or actual anndata objects\n",
    "    if isinstance(h5ad_inputs[0], str):\n",
    "        # If inputs are paths, read the .h5ad files and generate batch keys based on the directory names\n",
    "        to_concat = [sc.read_h5ad(input, backed='r') for input in h5ad_inputs]\n",
    "        batch_keys = [os.path.basename(os.path.dirname(input)) for input in h5ad_inputs]\n",
    "    else:\n",
    "        # If inputs are AnnData objects, use them directly and generate unique batch keys for each dataset\n",
    "        to_concat = h5ad_inputs\n",
    "        batch_keys = [f\"batch_{i}\" for i, adata in enumerate(h5ad_inputs)]\n",
    "\n",
    "    # Apply feature set standardization\n",
    "    to_concat = feature_set_standardization(to_concat, standardization_method)\n",
    "\n",
    "    # Ensure unique keys for concatenation\n",
    "    if len(batch_keys) != len(set(batch_keys)):\n",
    "        # If batch keys are not unique, create new unique batch keys\n",
    "        batch_keys = [f\"batch_{i}\" for i in range(len(to_concat))]\n",
    "\n",
    "    # Concatenate anndata objects\n",
    "    concat_anndata = ad.concat(\n",
    "        to_concat,\n",
    "        join=\"outer\",\n",
    "        label='batch',\n",
    "        keys=batch_keys,\n",
    "        index_unique=None  # This will speed up the concatenation process by not checking for unique indices\n",
    "    )\n",
    "\n",
    "    # Set the .var attribute of the concatenated AnnData object to be the same as the first input AnnData object\n",
    "    concat_anndata.var = to_concat[0].var\n",
    "\n",
    "    # If the sparse flag is True, convert the final matrix to sparse\n",
    "    if sparse:\n",
    "        concat_anndata.X = csr_matrix(concat_anndata.X)\n",
    "\n",
    "    return concat_anndata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will showcase the usage of our `concatenate_anndata` function. This function incorporates the `feature_set_standardization` function. The expected output is a correctly configured, concatenated isomatrix anndata object, ready for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standardizing anndata features via union:   0%|          | 0/10 [00:00<?, ?it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  10%|█         | 1/10 [00:00<00:04,  2.12it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  20%|██        | 2/10 [00:01<00:04,  1.87it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  30%|███       | 3/10 [00:01<00:03,  1.94it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  40%|████      | 4/10 [00:02<00:03,  1.86it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  50%|█████     | 5/10 [00:02<00:02,  1.92it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  60%|██████    | 6/10 [00:03<00:02,  1.95it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  70%|███████   | 7/10 [00:03<00:01,  1.93it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  80%|████████  | 8/10 [00:04<00:01,  1.87it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union:  90%|█████████ | 9/10 [00:04<00:00,  1.92it/s]/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/utils.py:292: UserWarning: X converted to numpy array with dtype object\n",
      "  warnings.warn(f\"{name} converted to numpy array with dtype {arr.dtype}\")\n",
      "Standardizing anndata features via union: 100%|██████████| 10/10 [00:05<00:00,  1.93it/s]\n",
      "/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/_core/anndata.py:1897: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "andata_concat = concatenate_anndata(anndata_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1000 × 50000\n",
       "    obs: 'batch'\n",
       "    var: 'geneId', 'transcriptId', 'nbExons'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "andata_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneId</th>\n",
       "      <th>transcriptId</th>\n",
       "      <th>nbExons</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transcriptId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSMUST00000000001_000000_000000.1</th>\n",
       "      <td>Gene_1</td>\n",
       "      <td>ENSMUST00000000001_000000_000000.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSMUST00000000002_000000_000000.1</th>\n",
       "      <td>Gene_1</td>\n",
       "      <td>ENSMUST00000000002_000000_000000.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSMUST00000000003_000000_000000.1</th>\n",
       "      <td>Gene_1</td>\n",
       "      <td>ENSMUST00000000003_000000_000000.1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSMUST00000000004_000000_000000.1</th>\n",
       "      <td>Gene_1</td>\n",
       "      <td>ENSMUST00000000004_000000_000000.1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSMUST00000000005_000000_000000.1</th>\n",
       "      <td>Gene_1</td>\n",
       "      <td>ENSMUST00000000005_000000_000000.1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSMUST00000000004_000690_000008.1</th>\n",
       "      <td>Gene_691</td>\n",
       "      <td>ENSMUST00000000004_000690_000008.1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSMUST00000000004_000064_000003.1</th>\n",
       "      <td>Gene_65</td>\n",
       "      <td>ENSMUST00000000004_000064_000003.1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSMUST00000000003_000096_000009.1</th>\n",
       "      <td>Gene_97</td>\n",
       "      <td>ENSMUST00000000003_000096_000009.1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSMUST00000000002_000427_000001.1</th>\n",
       "      <td>Gene_428</td>\n",
       "      <td>ENSMUST00000000002_000427_000001.1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSMUST00000000003_000938_000006.1</th>\n",
       "      <td>Gene_939</td>\n",
       "      <td>ENSMUST00000000003_000938_000006.1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      geneId  \\\n",
       "transcriptId                                   \n",
       "ENSMUST00000000001_000000_000000.1    Gene_1   \n",
       "ENSMUST00000000002_000000_000000.1    Gene_1   \n",
       "ENSMUST00000000003_000000_000000.1    Gene_1   \n",
       "ENSMUST00000000004_000000_000000.1    Gene_1   \n",
       "ENSMUST00000000005_000000_000000.1    Gene_1   \n",
       "...                                      ...   \n",
       "ENSMUST00000000004_000690_000008.1  Gene_691   \n",
       "ENSMUST00000000004_000064_000003.1   Gene_65   \n",
       "ENSMUST00000000003_000096_000009.1   Gene_97   \n",
       "ENSMUST00000000002_000427_000001.1  Gene_428   \n",
       "ENSMUST00000000003_000938_000006.1  Gene_939   \n",
       "\n",
       "                                                          transcriptId  \\\n",
       "transcriptId                                                             \n",
       "ENSMUST00000000001_000000_000000.1  ENSMUST00000000001_000000_000000.1   \n",
       "ENSMUST00000000002_000000_000000.1  ENSMUST00000000002_000000_000000.1   \n",
       "ENSMUST00000000003_000000_000000.1  ENSMUST00000000003_000000_000000.1   \n",
       "ENSMUST00000000004_000000_000000.1  ENSMUST00000000004_000000_000000.1   \n",
       "ENSMUST00000000005_000000_000000.1  ENSMUST00000000005_000000_000000.1   \n",
       "...                                                                ...   \n",
       "ENSMUST00000000004_000690_000008.1  ENSMUST00000000004_000690_000008.1   \n",
       "ENSMUST00000000004_000064_000003.1  ENSMUST00000000004_000064_000003.1   \n",
       "ENSMUST00000000003_000096_000009.1  ENSMUST00000000003_000096_000009.1   \n",
       "ENSMUST00000000002_000427_000001.1  ENSMUST00000000002_000427_000001.1   \n",
       "ENSMUST00000000003_000938_000006.1  ENSMUST00000000003_000938_000006.1   \n",
       "\n",
       "                                    nbExons  \n",
       "transcriptId                                 \n",
       "ENSMUST00000000001_000000_000000.1       10  \n",
       "ENSMUST00000000002_000000_000000.1        6  \n",
       "ENSMUST00000000003_000000_000000.1       18  \n",
       "ENSMUST00000000004_000000_000000.1       17  \n",
       "ENSMUST00000000005_000000_000000.1        9  \n",
       "...                                     ...  \n",
       "ENSMUST00000000004_000690_000008.1       13  \n",
       "ENSMUST00000000004_000064_000003.1        4  \n",
       "ENSMUST00000000003_000096_000009.1        4  \n",
       "ENSMUST00000000002_000427_000001.1        7  \n",
       "ENSMUST00000000003_000938_000006.1       16  \n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "andata_concat.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
