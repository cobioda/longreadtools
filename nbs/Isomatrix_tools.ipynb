{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isomatrix Tools\n",
    "\n",
    "> Tools for converting isomatrix files into anndata objects for integration with the Scanpy ecosystem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp isomatrix_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from scanpy import AnnData\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "\n",
    "def isomatrix_to_anndata(file_path:str,  # The path to the isomatrix csv file to be read.\n",
    "                        sparse:bool=True  # Flag to determine if the output should be a sparse matrix.\n",
    ") -> AnnData: # The converted isomatrix as a scanpy compatible  anndata object\n",
    "    \"\"\"\n",
    "    This function converts an isomatrix txt file (SiCeLoRe output) into an AnnData object compatible with scanpy\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read in the data from the file\n",
    "    df = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "    # Filter out rows where the transcriptId is \"undef\"\n",
    "    df = df.loc[df[\"transcriptId\"] != \"undef\"]\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    df = df.transpose()\n",
    "    \n",
    "    # Extract the rows with 'gene_id', 'transcript_id', 'nb_exons' from the DataFrame\n",
    "    additional_info_rows = df.loc[df.index.intersection(['geneId', 'transcriptId', 'nbExons'])]\n",
    "    # Drop 'gene_id', 'transcript_id', 'nb_exons' rows from the DataFrame if they exist\n",
    "    df = df.drop(['geneId', 'transcriptId', 'nbExons'], errors='ignore')\n",
    "\n",
    "    # Convert the DataFrame to a sparse matrix if the sparse flag is True\n",
    "    if sparse:\n",
    "        matrix = csr_matrix(df.values.astype('float32'))\n",
    "    else:\n",
    "        try:\n",
    "            matrix = df.values.astype('float32')\n",
    "        except ValueError:\n",
    "            print(\"Error: Non-numeric data present in the DataFrame. Cannot convert to float.\")\n",
    "            return None\n",
    "    \n",
    "    # Convert the matrix to an AnnData object\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        anndata = sc.AnnData(X=matrix, obs=pd.DataFrame(index=df.index), var=pd.DataFrame(index=df.columns))\n",
    "    \n",
    "    # Add additional information to the AnnData object vars\n",
    "    for info in ['geneId', 'transcriptId', 'nbExons']:\n",
    "        if info in additional_info_rows.index:\n",
    "            anndata.var[info] = additional_info_rows.loc[info, :].values\n",
    "            if info == 'nbExons':\n",
    "                anndata.var[info] = anndata.var[info].astype('int32')\n",
    "    \n",
    "    return anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_test_data() -> str: #The absolute path of the extracted file 'sample_isomatrix.txt' if the download is successful.\n",
    "    \"\"\"\n",
    "    This function downloads a test data file from a specified URL, saves it locally, and extracts it.\n",
    "    \"\"\"\n",
    "    import urllib.request\n",
    "    import gzip\n",
    "    import shutil\n",
    "    import os\n",
    "\n",
    "    # URL of the file to be downloaded\n",
    "    url = \"https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748087/suppl/GSM3748087%5F190c.isoforms.matrix.txt.gz\"\n",
    "\n",
    "    # Download the file from `url` and save it locally under `file.txt.gz`:\n",
    "    urllib.request.urlretrieve(url, 'file.txt.gz')\n",
    "\n",
    "    # Check if the file is downloaded correctly\n",
    "    if os.path.exists('file.txt.gz'):\n",
    "        print(\"File downloaded successfully\")\n",
    "        # Now we need to extract the file\n",
    "        with gzip.open('file.txt.gz', 'rb') as f_in:\n",
    "            with open('sample_isomatrix.txt', 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(\"File extracted successfully\")\n",
    "        return os.path.abspath('sample_isomatrix.txt')\n",
    "    else:\n",
    "        print(\"Failed to download the file\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of `isomatrix_to_anndata`: We can use the `download_test_data` function to download a small isoform matrix dataset for demonstrating the functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully\n",
      "File extracted successfully\n"
     ]
    }
   ],
   "source": [
    "from longreadtools.isomatrix_tools import * \n",
    "test_file = download_test_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "anndata = isomatrix_to_anndata(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the anndata object generated from the isomatrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneId</th>\n",
       "      <th>transcriptId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Klc2</td>\n",
       "      <td>ENSMUST00000156717.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Capn15</td>\n",
       "      <td>ENSMUST00000212520.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Klc2</td>\n",
       "      <td>ENSMUST00000025798.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eva1c</td>\n",
       "      <td>ENSMUST00000231280.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Atg5</td>\n",
       "      <td>ENSMUST00000039286.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20829</th>\n",
       "      <td>Kcnj9</td>\n",
       "      <td>ENSMUST00000062387.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20830</th>\n",
       "      <td>Iqcg</td>\n",
       "      <td>ENSMUST00000115100.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20831</th>\n",
       "      <td>Nt5dc2</td>\n",
       "      <td>ENSMUST00000227096.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20832</th>\n",
       "      <td>Emg1</td>\n",
       "      <td>ENSMUST00000004379.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20833</th>\n",
       "      <td>Kat6a</td>\n",
       "      <td>ENSMUST00000110696.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20834 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       geneId           transcriptId\n",
       "0        Klc2   ENSMUST00000156717.1\n",
       "1      Capn15   ENSMUST00000212520.1\n",
       "2        Klc2  ENSMUST00000025798.12\n",
       "3       Eva1c   ENSMUST00000231280.1\n",
       "4        Atg5   ENSMUST00000039286.4\n",
       "...       ...                    ...\n",
       "20829   Kcnj9   ENSMUST00000062387.7\n",
       "20830    Iqcg   ENSMUST00000115100.8\n",
       "20831  Nt5dc2   ENSMUST00000227096.1\n",
       "20832    Emg1   ENSMUST00000004379.7\n",
       "20833   Kat6a   ENSMUST00000110696.7\n",
       "\n",
       "[20834 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anndata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_isomatrix_to_anndata():\n",
    "    # Test with a known file\n",
    "    test_file = download_test_data()\n",
    "    anndata = isomatrix_to_anndata(test_file)\n",
    "\n",
    "    # Check the type of the returned object\n",
    "    assert isinstance(anndata, sc.AnnData), \"The returned object is not an AnnData object.\"\n",
    "\n",
    "    # Check the dimensions of the AnnData object\n",
    "    assert anndata.shape == (190, 20834), \"The dimensions of the AnnData object are not as expected.\"\n",
    "\n",
    "    # Check the var names of the AnnData object\n",
    "    assert 'geneId' in anndata.var, \"The 'geneId' is not in the var of the AnnData object.\"\n",
    "    assert 'transcriptId' in anndata.var, \"The 'transcriptId' is not in the var of the AnnData object.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully\n",
      "File extracted successfully\n"
     ]
    }
   ],
   "source": [
    "test_file = download_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, it may be necessary to convert more than one isomatrix in bulk. The function `multiple_isomatrix_conversion` has been designed for this purpose. It leverages Python's multiprocessing capabilities to perform this task in a fast and efficient manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np \n",
    "from pandas import DataFrame\n",
    "\n",
    "def simulate_isomatrix(num_genes: int, # number of genes (groups of rows)\n",
    "                       num_transcripts_per_gene: int, # number of transcripts per gene\n",
    "                       num_samples: int, # number of samples (columns)\n",
    "                       sparsity: float = 0.95, # fraction of zeros in the data (default 0.95)\n",
    "                       max_expression: int = 100, # maximum expression level for any transcript in any sample\n",
    "                       seed: int = 0 # random seed for reproducibility\n",
    "                      ) -> DataFrame : # DataFrame with simulated transcript expression data for demonstration purposes.\n",
    "    \"\"\"\n",
    "    Simulate transcript expression data to match the structure of the first image provided by the user.\n",
    "    Allows specifying the number of genes, transcripts per gene, and samples.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Calculate total number of transcripts\n",
    "    total_transcripts = num_genes * num_transcripts_per_gene\n",
    "    \n",
    "    # Generate random data\n",
    "    data = np.random.rand(total_transcripts, num_samples)\n",
    "    \n",
    "    # Apply sparsity\n",
    "    zero_mask = np.random.rand(total_transcripts, num_samples) > sparsity\n",
    "    data[~zero_mask] = 0  # Set a fraction of data to 0 based on sparsity\n",
    "    \n",
    "    # Scale data to have values up to max_expression\n",
    "    data = np.ceil(data * max_expression).astype(int)\n",
    "    \n",
    "    # Generate transcript and sample labels\n",
    "    transcript_ids = [f\"ENSMUST00000{str(i).zfill(6)}_{str(j).zfill(6)}_{str(seed).zfill(6)}.1\" for j in range(num_genes) for i in range(1, num_transcripts_per_gene + 1)]\n",
    "    gene_ids = [f\"Gene_{(i // num_transcripts_per_gene) + 1}\" for i in range(total_transcripts)]\n",
    "    nb_exons = np.random.randint(1, 21, total_transcripts)  # Assuming 1-20 exons based on typical gene structures\n",
    "    sample_ids = [f\"CACCTACACGTCAAC{str(i).zfill(2)}\" for i in range(1, num_samples + 1)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, index=gene_ids, columns=sample_ids)\n",
    "    df.index.name = 'geneId'  # Add index name\n",
    "    df.insert(0, 'transcriptId', transcript_ids)\n",
    "    df.insert(1, 'nbExons', nb_exons)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "def simulate_and_save_isomatrices(num_isomatrix: int, #number of isomatrix to generate\n",
    "                                num_genes: int, # number of genes (groups of rows)\n",
    "                                num_transcripts_per_gene: int, # number of transcripts per gene\n",
    "                                num_samples: int, # number of samples (columns)\n",
    "                                sparsity: float = 0.95, # fraction of zeros in the data (default 0.95)\n",
    "                                max_expression: int = 100, # maximum expression level for any transcript in any sample\n",
    "                                seed: int = 0, # random seed for reproducibility\n",
    "                                output_dir: str = './', # directory to save the generated isomatrix txt files\n",
    "                                return_paths: bool = False, # return paths to the isomatrixs as a list of strings if True\n",
    "                                verbose: bool = False # print progress messages if True\n",
    "                               ) -> list: # list of paths for the simulated matrices (if return is set True)\n",
    "    \n",
    "    \"\"\"\n",
    "    Simulate multiple isomatrix and save them as txt files in the specified directory.\n",
    "    If return_paths is True, return a list of paths to the saved isomatrix files.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_files = []\n",
    "    for i in range(num_isomatrix):\n",
    "        # Generate isomatrix\n",
    "        df = simulate_isomatrix(num_genes, num_transcripts_per_gene, num_samples, sparsity, max_expression, seed+i)\n",
    "        \n",
    "        # Save to txt file\n",
    "        output_file = os.path.join(output_dir, f'isomatrix_{i+1}.txt')\n",
    "        df.to_csv(output_file, sep='\\t')\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Isomatrix {i+1} saved to {output_file}')\n",
    "        output_files.append(output_file)\n",
    "    \n",
    "    if return_paths:\n",
    "        return output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def convert_and_save_file(sample, verbose):\n",
    "    anndata = isomatrix_to_anndata(sample)\n",
    "    h5ad_file = sample.replace('.txt', '.h5ad')\n",
    "    anndata.write_h5ad(h5ad_file)\n",
    "    if verbose:\n",
    "        print(f\"File {h5ad_file} was successfully written to disk.\")\n",
    "    return h5ad_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def multiple_isomatrix_conversion(file_paths: list, # A list of file paths to be converted.\n",
    "                                  verbose: bool = False, # If True, print progress messages.\n",
    "                                  return_paths: bool = False # If True, return a list of paths to the converted files.\n",
    "                                  ) -> list: # A list of paths to the converted files.\n",
    "    \"\"\"\n",
    "    This function takes a list of file paths, converts each file from isomatrix to anndata format, \n",
    "    and saves the converted file in the same location with the same name but with a .h5ad extension.\n",
    "    If return_paths is True, it returns a list of paths to the converted files.\n",
    "    \"\"\"\n",
    "    with Pool() as p:\n",
    "        converted_files = p.map(partial(convert_and_save_file, verbose=verbose), file_paths)\n",
    "    \n",
    "    if return_paths:\n",
    "        return converted_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to use the function to convert multiple Isomatrix objects simultaneously. The function requires a list of paths to the Isomatrix text files as input. To demonstrate this functionality and to aid further development by other contributors, the ability to simulate the Isomatrix data has been provided.\n",
    " \n",
    "In this section, we will be making use of the `simulate_and_save_isomatrices` function to generate and store 10 isomatrices. Following this, the `multiple_isomatrix_conversion` function will be employed to convert these isomatrices into anndata objects and subsequently save them to the disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# import os\n",
    "# import re\n",
    "\n",
    "# directory = '/data/analysis/data_mcandrew/000-sclr-discovair/'\n",
    "\n",
    "# # Define the regular expression pattern\n",
    "# pattern = re.compile('.*(_BIOP_INT|BIOP_NAS)$')\n",
    "\n",
    "# # Get a list of all files in the directory\n",
    "# all_files = os.listdir(directory)\n",
    "\n",
    "# # Filter the list to include only files that match the pattern\n",
    "# matching_files = [os.path.join(directory, f) for f in all_files if pattern.match(f)]\n",
    "\n",
    "# # Print the list of matching files\n",
    "# print(matching_files)\n",
    "\n",
    "# individual_runs = matching_files\n",
    "# individual_runs = [f'{run}_isomatrix.txt' for run in individual_runs]\n",
    "# individual_runs = [os.path.join(run, f'{os.path.basename(run)}_isomatrix.txt') for run in matching_files]\n",
    "\n",
    "# isomatrice_paths = individual_runs\n",
    "# anndata_paths = multiple_isomatrix_conversion(isomatrice_paths, verbose=True, return_paths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_set_var_names(path):\n",
    "    dataset = sc.read_h5ad(path, backed='r')  # Read the file in 'backed' mode to avoid loading the whole data into memory\n",
    "    dataset.var_names = dataset.var['transcriptId']\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's simulate and convert multiple isomatrices to showcase the functionality of our concatenation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isomatrix 1 saved to ./isomatrix_1.txt\n",
      "Isomatrix 2 saved to ./isomatrix_2.txt\n",
      "Isomatrix 3 saved to ./isomatrix_3.txt\n",
      "Isomatrix 4 saved to ./isomatrix_4.txt\n",
      "Isomatrix 5 saved to ./isomatrix_5.txt\n",
      "Isomatrix 6 saved to ./isomatrix_6.txt\n",
      "Isomatrix 7 saved to ./isomatrix_7.txt\n",
      "Isomatrix 8 saved to ./isomatrix_8.txt\n",
      "Isomatrix 9 saved to ./isomatrix_9.txt\n",
      "Isomatrix 10 saved to ./isomatrix_10.txt\n",
      "File ./isomatrix_8.h5ad was successfully written to disk.File ./isomatrix_10.h5ad was successfully written to disk.File ./isomatrix_9.h5ad was successfully written to disk.File ./isomatrix_4.h5ad was successfully written to disk.File ./isomatrix_3.h5ad was successfully written to disk.File ./isomatrix_5.h5ad was successfully written to disk.File ./isomatrix_2.h5ad was successfully written to disk.File ./isomatrix_6.h5ad was successfully written to disk.File ./isomatrix_7.h5ad was successfully written to disk.File ./isomatrix_1.h5ad was successfully written to disk.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simulation = simulate_and_save_isomatrices(num_isomatrix=10, num_genes=1000, num_transcripts_per_gene=5, num_samples=100, sparsity=0.95, max_expression=100, seed=0, output_dir='./', return_paths=True, verbose=True)\n",
    "anndata_paths = multiple_isomatrix_conversion(simulation, verbose=True, return_paths=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two other useful tools for working with isomatrix anndata. The first, `feature_set_standardization`, can take a list of isomatrix anndata objects and standardize the features. For instance, if we run the function with `standardization_method` set to 'union', it will return a list of anndata objects with the .var attribute being the union of all geneId, transcriptId, and nbExons. Any values missing in the original datasets will be set to zero. Alternatively, if we run it with `standardization_method` set to 'intersection', only the intersecting features will be retained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def feature_set_standardization(adatas:list, # list of AnnData objects or paths to AnnData files\n",
    "                                standardization_method:str = 'union' # str specifiying method to use union or intersection\n",
    "                                ) -> list: # list of anndata objects with the features standardised \n",
    "    \"\"\"\n",
    "    Standardize the feature sets of multiple AnnData objects.\n",
    "    \n",
    "    This function takes a list of AnnData objects or paths to AnnData files and a standardization method as input.\n",
    "    The standardization method can be either 'union' or 'intersection'.\n",
    "    If 'union' is chosen, the function will create a union of all features across all AnnData objects.\n",
    "    If 'intersection' is chosen, the function will create an intersection of all features across all AnnData objects.\n",
    "    The function returns a list of standardized AnnData objects.\n",
    "    \"\"\"\n",
    "    # Check if the first element in adatas is a string\n",
    "    if isinstance(adatas[0], str):\n",
    "        # If it is, load anndata objects from paths\n",
    "        adatas = Parallel(n_jobs=-1)(delayed(load_and_set_var_names)(path) for path in adatas)\n",
    "\n",
    "    all_features = set()\n",
    "    common_features = set()\n",
    "    \n",
    "    # Get union or intersection of all feature sets across all AnnData objects\n",
    "    if standardization_method == 'union':\n",
    "        all_features = set().union(*[set(adata.var.itertuples(index=False, name=None)) for adata in adatas])\n",
    "    elif standardization_method == 'intersection':\n",
    "        common_features = set.intersection(*[set(adata.var['transcriptId']) for adata in adatas])\n",
    "    else:\n",
    "        raise ValueError(\"standardization_method should be 'union' or 'intersection'\")\n",
    "    \n",
    "\n",
    "    standardized_adatas = []\n",
    "    # Iterate over each AnnData object\n",
    "    for dataset in tqdm(adatas, desc= f\"Standardizing anndata features via {standardization_method}\"):\n",
    "        dataset.var_names = dataset.var['transcriptId']\n",
    "        existing_var = dataset.var\n",
    "        # Identify features that are in the union/intersection but not in the current AnnData object\n",
    "        missing_features = all_features - set(dataset.var.itertuples(index=False, name=None))\n",
    "        if standardization_method == 'union':\n",
    "            if missing_features:\n",
    "                # Create a DataFrame of zeros with rows equal to the number of observations in the current AnnData object\n",
    "                # and columns equal to the number of missing features\n",
    "                zero_data = np.zeros((dataset.n_obs, len(missing_features)), dtype=object)\n",
    "                zero_df = pd.DataFrame(zero_data, index=dataset.obs_names, columns=pd.Index([t[1] for t in missing_features], name='transcriptId'))\n",
    "\n",
    "                # Merge the zero_df with the dataset's .to_df() DataFrame along the columns\n",
    "                combined_df = pd.concat([dataset.to_df(), zero_df], axis=1)\n",
    "\n",
    "                # Convert the combined DataFrame back to an AnnData object\n",
    "                combined_data = sc.AnnData(combined_df, obs=dataset.obs, var=pd.DataFrame(index=combined_df.columns))\n",
    "\n",
    "                missing_df = pd.DataFrame(list(missing_features), columns=['geneId', 'transcriptId', 'nbExons'])\n",
    "                missing_df.set_index('transcriptId', inplace=True)\n",
    "\n",
    "                combined_data.var = pd.concat([existing_var, missing_df], axis=0)\n",
    "                combined_data.var['transcriptId'] = combined_data.var_names\n",
    "                standardized_adatas.append(combined_data)\n",
    "            else:\n",
    "                # If no features are missing, append the dataset as is\n",
    "                standardized_adatas.append(dataset)\n",
    "        elif standardization_method == 'intersection':\n",
    "            # Subset the dataset to only include transcriptIds that are in the intersection\n",
    "            dataset = dataset[:, dataset.var_names.isin(common_features)]\n",
    "            standardized_adatas.append(dataset)\n",
    "    return standardized_adatas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the function on are simulated data and see what the result looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "h5py objects cannot be pickled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 370, in _sendback_result\n    result_queue.put(\n  File \"/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/joblib/externals/loky/backend/queues.py\", line 230, in put\n    obj = dumps(obj, reducers=self._reducers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/joblib/externals/loky/backend/reduction.py\", line 215, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/joblib/externals/loky/backend/reduction.py\", line 208, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n    return Pickler.dump(self, obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/h5py/_hl/base.py\", line 370, in __getnewargs__\n    raise TypeError(\"h5py objects cannot be pickled\")\nTypeError: h5py objects cannot be pickled\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feature_set_standardization(anndata_paths)\n",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m, in \u001b[0;36mfeature_set_standardization\u001b[0;34m(adatas, standardization_method)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Check if the first element in adatas is a string\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(adatas[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# If it is, load anndata objects from paths\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     adatas \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(delayed(load_and_set_var_names)(path) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m adatas)\n\u001b[1;32m     20\u001b[0m all_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     21\u001b[0m common_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/scLRanalyis/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/.conda/envs/scLRanalyis/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/scLRanalyis/lib/python3.11/site-packages/joblib/parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1693\u001b[0m \n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1699\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[1;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/scLRanalyis/lib/python3.11/site-packages/joblib/parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1734\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/.conda/envs/scLRanalyis/lib/python3.11/site-packages/joblib/parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/scLRanalyis/lib/python3.11/site-packages/joblib/parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: h5py objects cannot be pickled"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_set_standardization(anndata_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import anndata as ad\n",
    "\n",
    "def concatenate_anndata(h5ad_inputs: list, # A list of AnnData objects or paths to .h5ad files.\n",
    "                         standardization_method='union' # The method to standardize the feature sets across all AnnData objects. It can be either 'union' or 'intersection'. Default is 'union'.\n",
    "                         ) -> AnnData: # The concatenated AnnData object.\n",
    "    \"\"\"\n",
    "    This function concatenates multiple AnnData objects into a single AnnData object.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if inputs are paths or actual anndata objects\n",
    "    if isinstance(h5ad_inputs[0], str):\n",
    "        # If inputs are paths, read the .h5ad files and generate batch keys based on the directory names\n",
    "        to_concat = [sc.read_h5ad(input, backed='r') for input in h5ad_inputs]\n",
    "        batch_keys = [os.path.basename(os.path.dirname(input)) for input in h5ad_inputs]\n",
    "    else:\n",
    "        # If inputs are AnnData objects, use them directly and generate unique batch keys for each dataset\n",
    "        to_concat = h5ad_inputs\n",
    "        batch_keys = [f\"batch_{i}\" for i, adata in enumerate(h5ad_inputs)]\n",
    "\n",
    "    # Apply feature set standardization\n",
    "    to_concat = feature_set_standardization(to_concat, standardization_method)\n",
    "\n",
    "    # Ensure unique keys for concatenation\n",
    "    if len(batch_keys) != len(set(batch_keys)):\n",
    "        # If batch keys are not unique, create new unique batch keys\n",
    "        batch_keys = [f\"batch_{i}\" for i in range(len(to_concat))]\n",
    "\n",
    "    # Concatenate anndata objects\n",
    "    concat_anndata = ad.concat(\n",
    "        to_concat,\n",
    "        join=\"outer\",\n",
    "        label='batch',\n",
    "        keys=batch_keys,\n",
    "    )\n",
    "\n",
    "    # Set the .var attribute of the concatenated AnnData object to be the same as the first input AnnData object\n",
    "    concat_anndata.var = to_concat[0].var\n",
    "\n",
    "    return concat_anndata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will showcase the usage of our `concatenate_anndata` function. This function incorporates the `feature_set_standardization` function. The expected output is a correctly configured, concatenated isomatrix anndata object, ready for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standardizing anndata features via union: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.37it/s]\n",
      "/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/_core/anndata.py:1897: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "andata_concat = concatenate_anndata(anndata_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs Ã— n_vars = 1000 Ã— 50000\n",
       "    obs: 'batch'\n",
       "    var: 'geneId', 'transcriptId', 'nbExons'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "andata_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standardizing anndata features via intersection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 157.38it/s]\n",
      "/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/_core/anndata.py:183: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/mcandrew/.conda/envs/scLRanalyis/lib/python3.11/site-packages/anndata/_core/anndata.py:1897: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "andata_concat.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
