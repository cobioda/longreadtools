{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isomatrix Tools\n",
    "\n",
    "> A suite of utilities designed for handling isomatrices, which are generated by scisclor. These tools facilitate tasks such as converting isomatrices to AnnData format, merging multiple isomatrices, and more. Additionally, the suite includes features for downloading or generating isomatrix data, which can be particularly useful for testing and demonstrating new features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp isomatool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Isomatrix Tools module provides a comprehensive set of utilities for working with isomatrices, particularly those generated by the scisclor tool. One of the key functionalities of this module is the conversion of isomatrix text files, which are the output of scisclor, into AnnData objects that are compatible with the Scanpy library. This conversion process is crucial for enabling downstream analysis of single-cell long-read data within the Scanpy ecosystem, allowing users to leverage its powerful analytical capabilities. The module ensures that the conversion retains all necessary gene and transcript information, and it supports the creation of both dense and sparse matrix representations to cater to different computational needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import scanpy as sc\n",
    "import warnings\n",
    "import anndata as ad\n",
    "from anndata import AnnData\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from functools import partial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def isomatrix_to_anndata(file_path:str,  # The path to the isomatrix csv file to be read.\n",
    "                        sparse:bool=False  # Flag to determine if the output should be a sparse matrix.\n",
    ") -> AnnData: # The converted isomatrix as a scanpy compatible anndata object\n",
    "    \"\"\"\n",
    "    This function converts an isomatrix txt file (SiCeLoRe output) into an AnnData object compatible with scanpy\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read in the data from the file\n",
    "    df = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "    # Filter out rows where the transcriptId is \"undef\"\n",
    "    df = df.loc[df[\"transcriptId\"] != \"undef\"]\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    df = df.transpose()\n",
    "    \n",
    "    # Extract the rows with 'geneId', 'transcriptId', 'nbExons' from the DataFrame\n",
    "    additional_info_rows = df.loc[df.index.intersection(['geneId', 'transcriptId', 'nbExons'])]\n",
    "    # Drop 'geneId', 'transcriptId', 'nbExons' rows from the DataFrame if they exist\n",
    "    df = df.drop(['geneId', 'transcriptId', 'nbExons'], errors='ignore')\n",
    "\n",
    "    # Convert the DataFrame to a sparse matrix if the sparse flag is True\n",
    "    if sparse:\n",
    "        matrix = csr_matrix(df.values.astype('float32'))\n",
    "    else:\n",
    "        try:\n",
    "            matrix = df.values.astype('float32')\n",
    "        except ValueError:\n",
    "            print(\"Error: Non-numeric data present in the DataFrame. Cannot convert to float.\")\n",
    "            return None\n",
    "    \n",
    "    # Convert the matrix to an AnnData object\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # Ensure that the index and columns are converted to strings\n",
    "        obs_index_as_str = df.index.astype(str)\n",
    "        var_columns_as_str = df.columns.astype(str)\n",
    "        anndata = sc.AnnData(X=matrix, obs=pd.DataFrame(index=obs_index_as_str), var=pd.DataFrame(index=var_columns_as_str))\n",
    "    \n",
    "    # Add additional information to the AnnData object vars\n",
    "    for info in ['geneId', 'transcriptId', 'nbExons']:\n",
    "        if info in additional_info_rows.index:\n",
    "            anndata.var[info] = additional_info_rows.loc[info, :].astype(str).values\n",
    "    \n",
    "    # Ensure unique observation names\n",
    "    anndata.obs_names_make_unique()\n",
    "    \n",
    "    return anndata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This section pertains to the retrieval of sample data which is essential for testing and validating the functionality of the Isomatrix Tools module. The sample data is an isomatrix text file, which is a typical output of the SiCeLoRe pipeline, and is used to ensure that the conversion to an AnnData object is performed correctly. The downloaded file is also used in the demonstration and testing of other functions within this module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_test_data() -> str: #The absolute path of the extracted file 'sample_isomatrix.txt' if the download is successful.\n",
    "    \"\"\"\n",
    "    This function downloads a test data file from a specified URL, saves it locally, and extracts it.\n",
    "    \"\"\"\n",
    "\n",
    "    # URL of the file to be downloaded\n",
    "    url = \"https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748087/suppl/GSM3748087%5F190c.isoforms.matrix.txt.gz\"\n",
    "\n",
    "    # Download the file from `url` and save it locally under `file.txt.gz`:\n",
    "    urllib.request.urlretrieve(url, 'file.txt.gz')\n",
    "\n",
    "    # Check if the file is downloaded correctly\n",
    "    if os.path.exists('file.txt.gz'):\n",
    "        print(\"File downloaded successfully\")\n",
    "        # Now we need to extract the file\n",
    "        with gzip.open('file.txt.gz', 'rb') as f_in:\n",
    "            with open('sample_isomatrix.txt', 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(\"File extracted successfully\")\n",
    "        return os.path.abspath('sample_isomatrix.txt')\n",
    "    else:\n",
    "        print(\"Failed to download the file\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Isomatrix Tools module includes functionality to simulate an isomatrix, which is a matrix representation of transcript expression data. This simulated data can be used for testing and validation purposes within the Isomatrix Tools framework. The simulation is designed to mimic the structure and characteristics of real transcriptomic datasets, allowing users to generate data with specified parameters such as the number of genes, transcripts per gene, number of samples, sparsity of the matrix, and maximum expression levels. The simulate_isomatrix function in the code block below provides a practical example of how such data can be generated for use with Isomatrix Tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np \n",
    "from pandas import DataFrame\n",
    "\n",
    "def simulate_isomatrix(num_genes: int, # number of genes (groups of rows)\n",
    "                       num_transcripts_per_gene: int, # number of transcripts per gene\n",
    "                       num_samples: int, # number of samples (columns)\n",
    "                       sparsity: float = 0.95, # fraction of zeros in the data (default 0.95)\n",
    "                       max_expression: int = 100, # maximum expression level for any transcript in any sample\n",
    "                       seed: int = 0 # random seed for reproducibility\n",
    "                      ) -> DataFrame : # DataFrame with simulated transcript expression data for demonstration purposes.\n",
    "    \"\"\"\n",
    "    Simulate transcript expression data to match the structure of the first image provided by the user.\n",
    "    Allows specifying the number of genes, transcripts per gene, and samples.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Calculate total number of transcripts\n",
    "    total_transcripts = num_genes * num_transcripts_per_gene\n",
    "    \n",
    "    # Generate random data\n",
    "    data = np.random.rand(total_transcripts, num_samples)\n",
    "    \n",
    "    # Apply sparsity\n",
    "    zero_mask = np.random.rand(total_transcripts, num_samples) > sparsity\n",
    "    data[~zero_mask] = 0  # Set a fraction of data to 0 based on sparsity\n",
    "    \n",
    "    # Scale data to have values up to max_expression\n",
    "    data = np.ceil(data * max_expression).astype(int)\n",
    "    \n",
    "    # Generate transcript and sample labels\n",
    "    transcript_ids = [f\"ENSMUST00000{str(i).zfill(6)}_{str(j).zfill(6)}_{str(seed).zfill(6)}.1\" for j in range(num_genes) for i in range(1, num_transcripts_per_gene + 1)]\n",
    "    gene_ids = [f\"Gene_{(i // num_transcripts_per_gene) + 1}\" for i in range(total_transcripts)]\n",
    "    nb_exons = np.random.randint(1, 21, total_transcripts)  # Assuming 1-20 exons based on typical gene structures\n",
    "    sample_ids = [f\"CACCTACACGTCAAC{str(i).zfill(2)}\" for i in range(1, num_samples + 1)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, index=gene_ids, columns=sample_ids)\n",
    "    df.index.name = 'geneId'  # Add index name\n",
    "    df.insert(0, 'transcriptId', transcript_ids)\n",
    "    df.insert(1, 'nbExons', nb_exons)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The isomatrix_tools module provides functionality to simulate and generate multiple isomatrix datasets. These datasets are essential for testing and demonstrating the capabilities of downstream analysis tools. The simulation process involves creating synthetic gene expression data that closely resembles real-world isomatrix data structures. This includes the ability to specify the number of genes, transcripts per gene, samples, and control the sparsity and maximum expression levels of the generated data. The simulate_and_save_isomatrices function within this module is particularly useful for creating a series of isomatrix files, which can be saved to a specified directory for further use in pipeline testing or demonstration purposes. The function also offers options to return the file paths of the generated isomatrices and to output progress messages during the simulation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def simulate_and_save_isomatrices(num_isomatrix: int, #number of isomatrix to generate\n",
    "                                num_genes: int, # number of genes (groups of rows)\n",
    "                                num_transcripts_per_gene: int, # number of transcripts per gene\n",
    "                                num_samples: int, # number of samples (columns)\n",
    "                                sparsity: float = 0.95, # fraction of zeros in the data (default 0.95)\n",
    "                                max_expression: int = 100, # maximum expression level for any transcript in any sample\n",
    "                                seed: int = 0, # random seed for reproducibility\n",
    "                                output_dir: str = './', # directory to save the generated isomatrix txt files\n",
    "                                return_paths: bool = False, # return paths to the isomatrixs as a list of strings if True\n",
    "                                verbose: bool = False # print progress messages if True\n",
    "                               ) -> list: # list of paths for the simulated matrices (if return is set True)\n",
    "    \n",
    "    \"\"\"\n",
    "    Simulate multiple isomatrix and save them as txt files in the specified directory.\n",
    "    If return_paths is True, return a list of paths to the saved isomatrix files.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_files = []\n",
    "    for i in range(num_isomatrix):\n",
    "        # Generate isomatrix\n",
    "        df = simulate_isomatrix(num_genes, num_transcripts_per_gene, num_samples, sparsity, max_expression, seed+i)\n",
    "        \n",
    "        # Save to txt file\n",
    "        output_file = os.path.join(output_dir, f'isomatrix_{i+1}.txt')\n",
    "        df.to_csv(output_file, sep='\\t')\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Isomatrix {i+1} saved to {output_file}')\n",
    "        output_files.append(output_file)\n",
    "    \n",
    "    if return_paths:\n",
    "        return output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_and_save_file(sample: str, # path to the isomatrix txt file\n",
    "                          verbose: bool, # print progress messages if True\n",
    "                          sparse: bool = False # store the anndata object in sparse format if True\n",
    "                         ) -> str: # path to the converted h5ad file\n",
    "    \"\"\"\n",
    "    Convert an isomatrix txt file to an AnnData h5ad file and save it to disk.\n",
    "    If the file already exists, it is overwritten.\n",
    "    A delay and retry mechanism is implemented to handle file locking issues.\n",
    "    \"\"\"\n",
    "    anndata = isomatrix_to_anndata(sample, sparse=sparse)\n",
    "    h5ad_file = sample.replace('.txt', '.h5ad')\n",
    "    \n",
    "    # Check if the file already exists and delete it if it does\n",
    "    if os.path.exists(h5ad_file):\n",
    "        os.remove(h5ad_file)\n",
    "    \n",
    "    # Add a delay and retry mechanism to handle file locking issues\n",
    "    for attempt in range(10):\n",
    "        try:\n",
    "            anndata.write_h5ad(h5ad_file)\n",
    "            break\n",
    "        except BlockingIOError:\n",
    "            if attempt < 9:  # i.e. if this is not the last attempt\n",
    "                time.sleep(1)  # wait for 1 second before the next attempt\n",
    "            else:\n",
    "                raise  # re-raise the last exception if all attempts fail\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"File {h5ad_file} was successfully written to disk.\")\n",
    "    \n",
    "    return h5ad_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#he Isomatrix tools module provides a suite of functions designed to facilitate the conversion of isomatrix files into AnnData objects, which are suitable for downstream analysis in single-cell genomics workflows. This module includes a function `multiple_isomatrix_conversion` that efficiently processes batches of isomatrix files, leveraging parallel processing to expedite the conversion. The converted AnnData objects can optionally be stored in a sparse format to optimize memory usage. Additionally, the module contains functions for handling file I/O operations, such as checking for the existence of files and implementing a retry mechanism to address file locking issues during the write process. The module also includes functionality to standardize feature sets across multiple datasets, ensuring consistency in subsequent analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def multiple_isomatrix_conversion(file_paths: list, # A list of file paths to be converted.\n",
    "                                  verbose: bool = False, # If True, print progress messages.\n",
    "                                  return_paths: bool = False, # If True, return a list of paths to the converted files.\n",
    "                                  sparse: bool = False # If True, the anndata object will be stored in sparse format.\n",
    "                                  ) -> list: # A list of paths to the converted files.\n",
    "    \"\"\"\n",
    "    This function takes a list of file paths, converts each file from isomatrix to anndata format, \n",
    "    and saves the converted file in the same location with the same name but with a .h5ad extension.\n",
    "    If return_paths is True, it returns a list of paths to the converted files.\n",
    "    If sparse is True, the anndata object will be stored in sparse format.\n",
    "    \"\"\"\n",
    "    with Pool() as p:\n",
    "        converted_files = p.map(partial(convert_and_save_file, verbose=verbose, sparse=sparse), file_paths)\n",
    "    \n",
    "    if return_paths:\n",
    "        return converted_files\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def load_and_set_var_names(path):\n",
    "    dataset = sc.read_h5ad(path, backed='r')  # Read the file in 'backed' mode to avoid loading the whole data into memory\n",
    "    dataset.var_names = dataset.var['transcriptId']\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Isomatrix tools module includes advanced functionality for standardizing transcript features across multiple isoform matrix datasets. This process is crucial for ensuring that subsequent analyses are consistent and comparable. The module provides the option to standardize features by either taking the union or the intersection of all transcripts present in the datasets. The 'union' method combines all unique transcripts from each dataset, thus preserving the full range of features. In contrast, the 'intersection' method retains only those transcripts that are common to all datasets, which can be beneficial for comparative studies where only shared features are of interest. This flexibility allows researchers to tailor the standardization process to their specific analytical needs and the nature of their datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def feature_set_standardization(adatas:list, # list of AnnData objects or paths to AnnData files\n",
    "                                standardization_method:str = 'union' # str specifiying method to use union or intersection\n",
    "                                ) -> list: # list of anndata objects with the features standardised \n",
    "    \"\"\"\n",
    "    Standardize the feature sets of multiple AnnData objects.\n",
    "    \n",
    "    This function takes a list of AnnData objects or paths to AnnData files and a standardization method as input.\n",
    "    The standardization method can be either 'union' or 'intersection'.\n",
    "    If 'union' is chosen, the function will create a union of all features across all AnnData objects.\n",
    "    If 'intersection' is chosen, the function will create an intersection of all features across all AnnData objects.\n",
    "    The function returns a list of standardized AnnData objects.\n",
    "    \"\"\"\n",
    "    # Check if the first element in adatas is a string\n",
    "    if isinstance(adatas[0], str):\n",
    "        # If it is, load anndata objects from paths\n",
    "        adatas = [load_and_set_var_names(path) for path in adatas]\n",
    "        for adata in adatas:\n",
    "            if isinstance(adata.X, csr_matrix):\n",
    "                adata.X = adata.X.toarray()\n",
    "    all_features = set()\n",
    "    common_features = set()\n",
    "    \n",
    "    # Get union or intersection of all feature sets across all AnnData objects\n",
    "    if standardization_method == 'union':\n",
    "        all_features = set().union(*[set(adata.var.itertuples(index=False, name=None)) for adata in adatas])\n",
    "    elif standardization_method == 'intersection':\n",
    "        common_features = set.intersection(*[set(adata.var['transcriptId']) for adata in adatas])\n",
    "    else:\n",
    "        raise ValueError(\"standardization_method should be 'union' or 'intersection'\")\n",
    "    \n",
    "\n",
    "    standardized_adatas = []\n",
    "    # Iterate over each AnnData object\n",
    "    for dataset in tqdm(adatas, desc= f\"Standardizing anndata features via {standardization_method}\"):\n",
    "        dataset.var_names = dataset.var['transcriptId']\n",
    "        existing_var = dataset.var\n",
    "        # Identify features that are in the union/intersection but not in the current AnnData object\n",
    "        missing_features = all_features - set(dataset.var.itertuples(index=False, name=None))\n",
    "        if standardization_method == 'union':\n",
    "            if missing_features:\n",
    "                # Create a DataFrame of zeros with rows equal to the number of observations in the current AnnData object\n",
    "                # and columns equal to the number of missing features\n",
    "                zero_data = np.zeros((dataset.n_obs, len(missing_features)), dtype=np.float32)\n",
    "                zero_df = pd.DataFrame(zero_data, index=dataset.obs_names, columns=pd.Index([t[1] for t in missing_features], name='transcriptId'))\n",
    "\n",
    "                # Merge the zero_df with the dataset's .to_df() DataFrame along the columns\n",
    "                combined_df = pd.concat([dataset.to_df(), zero_df], axis=1)\n",
    "\n",
    "                # Convert the combined DataFrame back to an AnnData object\n",
    "                combined_data = sc.AnnData(combined_df, obs=dataset.obs, var=pd.DataFrame(index=combined_df.columns))\n",
    "\n",
    "                missing_df = pd.DataFrame(list(missing_features), columns=['geneId', 'transcriptId', 'nbExons'])\n",
    "                missing_df.set_index('transcriptId', inplace=True)\n",
    "\n",
    "                combined_data.var = pd.concat([existing_var, missing_df], axis=0)\n",
    "                combined_data.var['transcriptId'] = combined_data.var_names\n",
    "                standardized_adatas.append(combined_data)\n",
    "            else:\n",
    "                # If no features are missing, append the dataset as is\n",
    "                standardized_adatas.append(dataset)\n",
    "        elif standardization_method == 'intersection':\n",
    "            # Subset the dataset to only include transcriptIds that are in the intersection\n",
    "            dataset = dataset[:, dataset.var_names.isin(common_features)]\n",
    "            standardized_adatas.append(dataset)\n",
    "    return standardized_adatas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Isomatrix tools module includes a function to validate and prepare an AnnData object for serialization and storage. This function ensures that the AnnData object conforms to the requirements for writing to disk as an .h5ad file, which is the file format used by Scanpy's `write_h5ad` method. The function checks for the presence of missing values in the `.var` and `.obs` DataFrames, converts non-string categorical data to strings, ensures that the observation and variable names are of string data type, and verifies that there are no duplicate names. Additionally, it checks for NaN values in the `.X` attribute, which holds the main data matrix, and provides warnings if any issues are detected that could interfere with the file writing process. This preprocessing step is crucial for maintaining data integrity and ensuring compatibility with downstream analysis tools that rely on the .h5ad file format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def check_anndata_for_saving(adata: AnnData, # The AnnData object to check.\n",
    "                               verbose: bool = False # If True, print status messages. Defaults to False.\n",
    "                               ):\n",
    "    \"\"\"\n",
    "    Prepare an AnnData object for saving by ensuring proper data types and handling missing values.\n",
    "    \"\"\"\n",
    "    # Function to check and convert data types in a DataFrame\n",
    "    def convert_df(df):\n",
    "        \"\"\"\n",
    "        Check and convert the data types of a DataFrame, filling missing values.\n",
    "        \n",
    "        Parameters:\n",
    "        df (DataFrame): The DataFrame to process.\n",
    "        \"\"\"\n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().any():\n",
    "                if verbose:\n",
    "                    print(f\"Missing values found in '{col}'. Filling with 'unknown' or median.\")\n",
    "                if pd.api.types.is_categorical_dtype(df[col]) or pd.api.types.is_object_dtype(df[col]):\n",
    "                    df[col] = df[col].astype('category').cat.add_categories(['unknown']).fillna('unknown')\n",
    "                else:\n",
    "                    df[col] = df[col].fillna(df[col].median())\n",
    "            if pd.api.types.is_object_dtype(df[col]):\n",
    "                df[col] = df[col].astype(str)\n",
    "            if pd.api.types.is_categorical_dtype(df[col]):\n",
    "                df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Process .var DataFrame if it exists\n",
    "    if adata.var is not None:\n",
    "        if verbose:\n",
    "            print(\"Processing .var DataFrame\")\n",
    "        convert_df(adata.var)\n",
    "    \n",
    "    # Process .obs DataFrame if it exists\n",
    "    if adata.obs is not None:\n",
    "        if verbose:\n",
    "            print(\"Processing .obs DataFrame\")\n",
    "        convert_df(adata.obs)\n",
    "    \n",
    "    # Ensure observation and variable names are strings\n",
    "    adata.obs_names = adata.obs_names.astype(str)\n",
    "    adata.var_names = adata.var_names.astype(str)\n",
    "\n",
    "    # Check for and warn about duplicate names\n",
    "    if adata.obs_names.duplicated().any() and verbose:\n",
    "        print(\"Duplicate obs_names found, consider making them unique.\")\n",
    "    if adata.var_names.duplicated().any() and verbose:\n",
    "        print(\"Duplicate var_names found, consider making them unique.\")\n",
    "    \n",
    "    # Check if .X contains NaN values and handle if necessary, accounting for sparse matrices\n",
    "    if issparse(adata.X):\n",
    "        if np.isnan(adata.X.data).any() and verbose:\n",
    "            print(\"NaN values found in sparse .X data. Consider handling them.\")\n",
    "            # Handle NaN values in sparse matrix data if necessary\n",
    "    else:\n",
    "        if np.isnan(adata.X).any() and verbose:\n",
    "            print(\"NaN values found in .X, consider handling them.\")\n",
    "            # Handle NaN values in .X if necessary\n",
    "    \n",
    "    # Final message if verbose\n",
    "    if verbose:\n",
    "        print(\"Preparation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module provides functionality to concatenate multiple AnnData objects, which are essential data structures for storing large-scale single-cell genomics data. The concatenation process is designed to combine data from different batches or experiments, while ensuring that the resulting AnnData object maintains the integrity and structure necessary for downstream analysis. The Isomatrix tools facilitate this process by handling discrepancies in data types, filling missing values, and standardizing feature sets across all input datasets. This ensures that the concatenated dataset is ready for high-throughput computational analyses, such as clustering, visualization, and differential expression testing, which are common in single-cell genomics workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_unique_batch_keys(batch_keys):\n",
    "    unique_keys = set()\n",
    "    final_keys = []\n",
    "    for key in batch_keys:\n",
    "        original_key = key\n",
    "        counter = 1\n",
    "        while key in unique_keys:\n",
    "            key = f\"{original_key}_{counter}\"\n",
    "            counter += 1\n",
    "        unique_keys.add(key)\n",
    "        final_keys.append(key)\n",
    "    return final_keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def concatenate_anndata(h5ad_inputs: list, # A list of AnnData objects or paths to .h5ad files.\n",
    "                         standardization_method='union', # The method to standardize the feature sets across all AnnData objects. It can be either 'union' or 'intersection'. Default is 'union'.\n",
    "                         sparse=False, # Optional flag to convert the final matrix to sparse. Default is False.\n",
    "                         verbose=False # Optional flag to print progress updates. Default is False.\n",
    "                         ) -> ad.AnnData: # The concatenated AnnData object.\n",
    "    \"\"\"\n",
    "    This function concatenates multiple AnnData objects into a single AnnData object and adds batch keys to identify the origin of each sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if inputs are paths or actual anndata objects\n",
    "    if isinstance(h5ad_inputs[0], str):\n",
    "        if verbose: print(\"Reading .h5ad files...\")\n",
    "        to_concat = [ad.read_h5ad(input) for input in h5ad_inputs]\n",
    "        batch_keys = [os.path.basename(os.path.dirname(input)) for input in h5ad_inputs]\n",
    "    else:\n",
    "        if verbose: print(\"Processing AnnData objects...\")\n",
    "        to_concat = h5ad_inputs\n",
    "        batch_keys = [os.path.basename(os.path.dirname(input)) for input in h5ad_inputs]\n",
    "\n",
    "    # Ensure batch keys are unique, append a UID if necessary\n",
    "    batch_keys = make_unique_batch_keys(batch_keys)\n",
    "\n",
    "    # If .X is sparse, convert to dense\n",
    "    for adata in to_concat:\n",
    "        if isinstance(adata.X, csr_matrix):\n",
    "            adata.X = adata.X.toarray()\n",
    "\n",
    "    # Apply feature set standardization\n",
    "    if verbose: print(\"Applying feature set standardization...\")\n",
    "    to_concat = feature_set_standardization(to_concat, standardization_method)\n",
    "\n",
    "    # Concatenate anndata objects with scanpy, specifying batch information\n",
    "    if verbose: print(\"Concatenating AnnData objects and adding batch keys with scanpy...\")\n",
    "    concat_anndata = ad.concat(to_concat, join='outer', label='batch', keys=batch_keys)\n",
    "\n",
    "    # Set the .var attribute of the concatenated AnnData object to be the same as the first input AnnData object\n",
    "    if verbose: print(\"Setting .var attribute...\")\n",
    "    concat_anndata.var = to_concat[0].var\n",
    "    concat_anndata.var = concat_anndata.var.astype(str)\n",
    "    concat_anndata.obs = concat_anndata.obs.astype(str)\n",
    "\n",
    "    # If the sparse flag is True, convert the final matrix to sparse\n",
    "    if sparse:\n",
    "        if verbose: print(\"Converting matrix to sparse...\")\n",
    "        try:\n",
    "            concat_anndata.X = csr_matrix(concat_anndata.X.astype(np.float32))\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting to sparse matrix: {e}\")\n",
    "\n",
    "    # Convert the data to float32 to avoid TypeError when writing to .h5ad file\n",
    "    concat_anndata.X = concat_anndata.X.astype(np.float32)\n",
    "    \n",
    "    if verbose: print(\"Final Check...\")\n",
    "    check_anndata_for_saving(concat_anndata)\n",
    "    \n",
    "\n",
    "    if verbose: print(\"Concatenation complete.\")\n",
    "    return concat_anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scLRanalyis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
